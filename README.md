# ğŸ¤– Ocular Agent

An intelligent web automation agent that uses **vision AI** and **adaptive planning** to navigate and interact with websites autonomously.

## âœ¨ Features

- ğŸ¯ **Adaptive Planning**: Creates plans and adjusts based on real-time feedback
- ğŸ‘ï¸ **Vision-Based Navigation**: Uses Qwen2.5-VL to understand web pages visually
- ğŸ”„ **Self-Correcting**: Verifies actions and replans if something goes wrong
- ğŸ¨ **Set-of-Mark (SoM)**: Visual element identification with bounding boxes
- ğŸš€ **Natural Language Commands**: Just describe what you want in plain English

## ğŸ¬ Demo

```bash
ğŸ‘‰ Goal: open blinkit and add 5 small maggi packets to cart
```

The agent will:
1. Navigate to Blinkit
2. Find and click the search box
3. Type "maggi"
4. Click on the product
5. Increase quantity to 5
6. Verify each step and replan if needed

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (GTX 1650 or better)
- 6GB+ VRAM

### Setup

1. Clone the repository:
```bash
git clone https://github.com/YOUR_USERNAME/ocular-agent.git
cd ocular-agent
```

2. Create virtual environment:
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac
```

3. Install dependencies:
```bash
pip install -r requirements.txt
playwright install chromium
```

4. Configure API keys:
```bash
# Copy .env.example to .env and add your keys
cp .env.example .env
```

Edit `.env` and add your DeepSeek API key:
```
DEEPSEEK_API_KEY=sk-your-key-here
```

## ğŸš€ Usage

Run the agent:
```bash
python main.py
```

### Execution Modes

**Mode 3 - Adaptive (Recommended):**
- Creates initial plan
- Verifies each step
- Replans if actions fail

**Mode 2 - Reactive:**
- No initial plan
- Decides next action based on current screen

**Mode 1 - Pre-planned:**
- Creates full plan upfront
- Executes without verification

### Example Commands

```
search for football shoes on amazon
go to flipkart and find gaming mouse
open blinkit and add 5 maggi packets to cart
```

## ğŸ—ï¸ Architecture

```
ocular-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py       # Task planning and adaptive logic
â”‚   â”œâ”€â”€ browser.py     # Playwright browser control
â”‚   â”œâ”€â”€ vision.py      # Qwen2.5-VL vision model
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ assets/            # Debug screenshots
â”œâ”€â”€ main.py           # Main entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

## ğŸ§  How It Works

1. **Planning**: DeepSeek creates a step-by-step plan
2. **Vision**: Qwen2.5-VL identifies elements on screen
3. **Execution**: Playwright performs browser actions
4. **Verification**: After each step, checks if it succeeded
5. **Adaptation**: If failed, creates new plan and retries

## ğŸ”§ Configuration

### Vision Model
Default: `Qwen/Qwen2.5-VL-3B-Instruct` (6GB VRAM)

For better accuracy (needs more VRAM):
- Edit `src/vision.py` to use `Qwen2.5-VL-7B-Instruct`

### Planning Model
Default: DeepSeek API

Alternatives in `.env`:
```bash
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4

# Local LLM (Ollama)
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_MODEL=llama3
```

## ğŸ“ Requirements

- torch>=2.4.0
- transformers>=4.46.0
- playwright>=1.48.0
- openai>=1.0.0
- python-dotenv

See `requirements.txt` for full list.

## ğŸ¤ Contributing

Contributions welcome! Feel free to:
- Report bugs
- Suggest features
- Submit pull requests

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2-VL) for vision capabilities
- [Playwright](https://playwright.dev/) for browser automation
- [DeepSeek](https://www.deepseek.com/) for planning intelligence

## âš ï¸ Disclaimer

This tool is for educational and research purposes. Always respect website terms of service and rate limits.
